\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{color}
\renewcommand\qedsymbol{$\blacksquare$}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}

\usepackage{hyperref}
\newcommand{\probref}[1]{Problem~\ref{#1}}
\newcommand{\cororef}[1]{Corollary~\ref{#1}}
\newcommand{\claimref}[1]{claim~\ref{#1}}
\newcommand{\Lemmaref}[1]{Lemma~\ref{#1}}

\newcommand{\x}{{\mathbf x}}
\renewcommand{\u}{{\mathbf u}}
\renewcommand{\v}{{\mathbf v}}
\newcommand{\e}{{\mathbf e}}

\newcommand{\norm}[1]{\left\Vert #1\right\Vert}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\orth}[2]{\cO^{#1\times #2}}
\newcommand{\mat}[2]{\R^{#1\times #2}}
\newcommand{\R}{\mathbb{R}}

\oddsidemargin=0.15in
\evensidemargin=0.15in
\topmargin=-.5in
\textheight=9in
\textwidth=6.25in

\title{Orthogonal Case}
\author{Gad Zalcberg}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\subsubsection{Notation}
\begin{itemize}
    \item Denote $\orth{d}{r}$ the set of the matrices with $r$ orthonormal columns in $\R^d$.
    \item I used upper case letters for matrices ($U,V$ for example), bold lower case letters for vectors ($\x,\e$ for example), and lower case letters for scalars (for example $x,e$)
    \item I used pythonic notation for indices:
    $U_{i:}$ for the $i$'th row, $U_{:j}$ for the $j$'th column, $U_{ij}$ for the $i,j$'th entry of matrix, and $\x_{i}$ for the $i$'th entry of vector.
    \item Denote $\{\e_i\}_{i=1}^d$ the standard basis for $\R^d$.
\end{itemize}

\subsection{Problem definition - scaled standard basis}
Given $\{\x_i\}_{i=1}^n\subset\R^d$, $n\le d$. Assume $\x_i\perp \x_j$ for all $i\ne j$ (not necessarily the standard basis, not necessarily unit norm). 

\subsubsection{Definitions}
\begin{itemize}
    \item Complete the samples $\{\x_i\}_{i=1}^n$ to orthonormal basis for $\R^d$: Let $V=\left(\frac{\x_1}{\Vert \x_1\Vert},...,\frac{\x_n}{\Vert \x_n\Vert}, \x_{n+1},...,\x_d\right)$ (where $\frac{\x_1}{\Vert \x_1\Vert},...,\x_d$ is orthonormal vectors).
    \item Let $\x_i\in\{\x_i\}_{i=1}^n$, we define $\Tilde{\x}_i:=V^T\x_i$ (note that: $\Tilde{\x}_i\in span(\e_i)$, i.e. $\frac{\Tilde{\x}_i}{\norm{\Tilde{\x}_i}}=\frac{\Tilde{\x}_i}{\norm{{\x}_i}}=\e_i$).
    \item Let  $U\in\mathbb{R}^{d\times r}$, we define $\hat{U}\in\mathbb{R}^{d\times r}$ by: $\hat{U}=V^TU,\quad V\hat{U}=U$.
    \item Denote $f(U,\x_i):=\left\Vert\x_{i}-UU^{T}\x_{i}\right\Vert^{2}-\left\Vert\x_{i}\right\Vert ^{2}$, and note that: $$f(U,\x_i)=\norm{\x_{i}-UU^{T}\x_{i}}^2-\norm{\x_{i}}^2=
    \norm{\Tilde{\x}_{i}-\hat{U}\hat{U}^{T}\Tilde{\x}_{i}}^2-\norm{\Tilde{\x}_{i}}^2=f(\hat{U},\Tilde{\x}_i)$$
    \item $g(U):=\max_if(U,\x_i)$
\end{itemize}
We are trying to solve:

\begin{equation} \label{factorized_problem}
\max_{\begin{array}{c}
U\in\R^{d\times r}\\
0\preceq UU^T\preceq I
\end{array}}\min_{{i}}\x_{i}^{T}UU^T\x_{i}
\end{equation}

Observe that for any $U\in\mathcal{O}^{n\times r}$ (by \Lemmaref{local minimum orthogonal} the optimal solution is in this set) we get:
\begin{align*}
    \left\Vert \x_i-UU^T\x_i\right\Vert^2- \left\Vert \x_{i}\right\Vert ^{2}
    =&\left\Vert \x_{i}\right\Vert^2-2\x_{i}^TUU^T\x_{i}+\x_{i}^TUU^TUU^T\x_{i}- \left\Vert \x_{i}\right\Vert^{2}\\
    =&-2\x_{i}^TUU^T\x_{i}+\x_{i}^TUU^T\x_{i}\\
    =&-\x_{i}^TUU^T\x_{i} 
\end{align*}

So we can solve equivalently:

\begin{equation} \label{factorized_problem_restore}
\min_{\begin{array}{c}
U\in\mathbb{R}^{d\times r}
\end{array}}\max_{i}\left\Vert \x_{i}-UU^T\x_{i}\right\Vert ^{2} - \left\Vert \x_{i}\right\Vert ^{2}=
\min_{\begin{array}{c}
U\in\mathbb{R}^{d\times r}
\end{array}}g(U)
\end{equation}


\subsection{Any local minimum of $g$ is globally optimal}

\begin{lemma} \label{not equal implies can improve}
Let $U$ such that: $f(U,\x_i)>f(U,\x_j)$ for some $i\ne j$, then for any $\epsilon>0$ there exist $U_{\theta}$ such that:
\begin{enumerate}
    \item $\left\Vert U-U_{\theta}\right\Vert \le\epsilon$.
    \item $f(U_{\theta},\x_j)<f(U,\x_i)$
    \item $\forall k\in[n]\setminus\left\{ i,j\right\} :\qquad f(U,\x_k)=f(U_{\theta},\x_k)$.
    \item $f(U_{\theta},\x_i)<f(U,\x_i)$
\end{enumerate}
\end{lemma}

\begin{proof}
We will use Given Rotation in order to find $U_\theta$. Let $U$ as above. Define $R(\theta)$, a rotation matrix (for some $\theta$) over the $i,j$ axes, i.e.:
$$
R(\theta)_{lm}=\begin{cases}
\cos\theta & lm=ii\quad or\quad lm=jj\\
\sin\theta & lm=ij\\
-\sin\theta & lm=ji\\
I_{lm} & else
\end{cases}
$$
and define $U_{\theta}=VR(\theta)V^TU$\\

\textbf{1+2 is true, since:} 

The functions $h_1\left(\theta\right):=U_{\theta},\quad h_2(\theta):=f(U_{\theta}, \x_j)$ are continuous functions, thus $\forall\epsilon\exists\delta$ such that $|\theta| \le\delta\Rightarrow$ 
\begin{align*}
  \left\Vert U_{\theta}- U\right\Vert = \left\Vert h_1(\theta)-h_1(0)\right\Vert& \le\epsilon  \\
  \left|f(U_{\theta},\x_j)- f(U,\x_j)\right|=\left|h_2(\theta)-h_2(0)\right|\le\epsilon&\Rightarrow f(U_{\theta},\x_j)<f(U,\x_j) + \epsilon < f(U,\x_i)
\end{align*}


\textbf{3 is true, since:} \\
For all $k\in[n]\setminus\left\{ i,j\right\}$: $\e_k=R(\theta)\e_k$, thus $VR(\theta)V^T\x_k=VR(\theta)(\norm{\x_k}\e_k)=V(\norm{\x_k}\e_k)=\x_k$ and:
\begin{align*}
\left\Vert \x_{k}-U_{\theta}U_{\theta}^{T}\x_{k}\right\Vert^{2}
=&\left\Vert\x_{k}-VR(\theta)V^TUU^{T}VR(\theta)^{T}V^T\x_{k}\right\Vert^{2}\\
=&\left\Vert VR(\theta)V^T\x_{k}-VR(\theta)V^TUU^{T}\x_{k}\right\Vert^{2}\\
=&\left\Vert\x_{k}-UU^{T}\x_{k}\right\Vert^{2}
\end{align*}

\textbf{4 is true, since:} 

Observe that\footnote{the proof of the last equation is in appendix \ref{technical proof}, since it is quite technical}:
\begin{align*}
    f(U_{\theta},\x_i)-f(U,\x_i)
    =&\left(\left\Vert\x_{i}-U_{\theta}U_{\theta}^{T}\x_{i}\right\Vert^{2}-\left\Vert\x_{i}\right\Vert^{2}\right)-\left(\left\Vert\x_{i}-UU^{T}\x_{i}\right\Vert^{2}-\left\Vert\x_{i}\right\Vert^{2}\right)\\
    =&\left\Vert\x_{i}-VR(\theta)V^TUU^{T}VR(\theta)^{T}V^T\x_{i}\right\Vert^{2}-\left\Vert V^T\x_{i}-V^TUU^{T}\x_{i}\right\Vert^{2}\\
    =&\left\Vert V^T\x_{i}-V^TVR(\theta)V^TUU^{T}VR(\theta)^{T}V^T\x_{i}\right\Vert^{2}-\left\Vert V^T\x_{i}-V^TUU^{T}V\Tilde{\x}_{i}\right\Vert^{2}\\
    =&\left\Vert \Tilde{\x}_{i}-R(\theta)\hat{U}\hat{U}^TR(\theta)^{T}\Tilde{\x}_{i}\right\Vert^{2}-\left\Vert \Tilde{\x}_{i}-\hat{U}\hat{U}^{T}\Tilde{\x}_{i}\right\Vert^{2}\\
    =&\left\Vert\Tilde{\x}_{i}\right\Vert^{2}\left(\left\Vert\e_{i}-R(\theta)\hat{U}\hat{U}^TR(\theta)^{T}\e_{i}\right\Vert ^{2}-\left\Vert\e_{i}-\hat{U}\hat{U}^{T}\e_{i}\right\Vert^{2}\right)\\
    =& \left\Vert\x_{i}\right\Vert^{2}\left(\sin\left(2\theta\right)\e_{i}^{T}\hat{U}\hat{U}^{T}\e_{j}^{T}+\sin\left(\theta\right)^{2}\left(\left\Vert\e_{j}-\hat{U}\hat{U}^{T}\e_{j}\right\Vert^{2}-\left\Vert\e_{i}-\hat{U}\hat{U}^{T}\e_{i}\right\Vert^{2}\right)\right)
\end{align*}

Now, $\sin\left(\theta\right)^{2}\left(\left\Vert\e_{j}-\hat{U}\hat{U}^{T}\e_{j}\right\Vert^{2}-\left\Vert\e_{i}-\hat{U}\hat{U}^{T}\e_{i}\right\Vert^{2}\right)<0$ for small $|\theta|$, so:

\begin{itemize}
    \item If: $\e_{i}^{T}\hat{U}\hat{U}^{T}\e_{j}^{T} \ge 0$ then for any $-\frac{\pi}{2}<\theta<0$:  $\sin\left(2\theta\right)\e_{i}^{T}\hat{U}\hat{U}^{T}\e_{j}^{T}<0$.
    \item If: $\e_{i}^{T}\hat{U}\hat{U}^{T}\e_{j}^{T} < 0$ then for any $\frac{\pi}{2}>\theta>0$:  $\sin\left(2\theta\right)\e_{i}^{T}\hat{U}\hat{U}^{T}\e_{j}^{T}<0$.
\end{itemize}
i.e. we can pick arbitrarily small $|\theta|$ with $f(U_{\theta},\x_i)-f(U,\x_i)<0$.
\end{proof}


\begin{lemma} \label{local minimum equals}
Let $U$ a local minimum of $g$, then for all $i$: 
$$f(U,\x_i)=g(U)$$
\end{lemma}
\begin{proof}
Assume there is $i$ with $f(U,\x_i)<g(U)$. W.L.O.G assume: $\{1...K\}=\arg\max_jf(U,\x_j)$, by \Lemmaref{not equal implies can improve}: 
\begin{itemize}
    \item There is $U_{1}$ with $\left\Vert U_{1}-U\right\Vert\le\frac{\epsilon}{K}$, $f(U_1,\x_k)<g(U)$ for all $k\ge K$, $f(U_1,\x_k)=g(U)$ for all $k< K$. 
    \item There is $U_{2}$ with $\left\Vert U_{2}-U_{1}\right\Vert\le\frac{\epsilon}{K}$, $f(U_2,\x_k)<g(U)$ for all $k\ge K-1$, $f(U_2,\x_k)=g(U)$ for all $k< K-1$.
    \item ...
    \item There is $U_{K}$ with $\left\Vert U_{K}-U_{K-1}\right\Vert\le\frac{\epsilon}{K}$, $f(U_K,\x_k)<g(U)$ for all $k$. 
\end{itemize}
Finally, observe that $\left\Vert U-U_{K}\right\Vert\le{\epsilon}$ so we can find arbitrarily close $U_K$ such that $g(U_{K})<g(U)$ in contradiction to the fact that $U$ is local minimum, so we conclude that the claim holds.
\end{proof}

\begin{lemma} \label{local minimum orthogonal}
Let $U$ a local minimum of $g$, then $U\in\mathcal{O}^{d\times r}$.
\end{lemma}

\begin{proof}
We use the following equation (proof is in appendix \ref{technical proof}): Let $ODO^{T}$ an EVD decomposition of $UU^{T}$, then:
\begin{align*}\label{}
    f_i(U)=\left\Vert\x_{i}-UU^{T}\x_{i}\right\Vert^{2} - \left\Vert\x_{i}\right\Vert^{2}=\left\Vert\x_{i}\right\Vert^{2}\sum_{l=1}^{r}(D_{ll}^{2}-2D_{ll})O_{li}^{2}
\end{align*}

Assume that there is $\hat{l}\le r$ such that ${D}_{\hat{l},\hat{l}}\ne1$ assume W.L.O.G ${D}_{\hat{l},\hat{l}}<1$. $O$ is full rank, so there exist $i$ such that $O_{\hat{l}i}\ne0$  and thus $({D}_{\hat{l},\hat{l}}^2-2{D}_{\hat{l},\hat{l}})O_{\hat{l}i}^{2}<(({D}_{\hat{l},\hat{l}}+\epsilon)^2-2({D}_{\hat{l},\hat{l}}+\epsilon))O_{\hat{l}i}$ for arbitrary small epsilon (we increase $D_{ll}$ by increasing the $l$'th singular values of $U$). This means that there is an arbitrarily close $U{'}$ with $g(U^{'})=f(U,\x_i)>f(U^{'},\x_i)$ and by \Lemmaref{not equal implies can improve} there is also an arbitrarily close $U{''}$ with $g({U})=g(U^{'})>g(U^{''})$. So we conclude that $\forall l: \quad D_{ll}=1$ i.e. $U$ has orthonormal columns. 
\end{proof}

\begin{claim} \label{local minimum is global}
Let $U$ a local minimum of $g$, then: 
\begin{itemize}
    \item $U$ is global minimum.
    \item $g(U)=\frac{-r}{\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}$.
\end{itemize}
\end{claim}

\begin{proof}

Let $U$ a local minimum of $g$, then for all $i\in[n]$:
$$
g(U)=^1f(U,\x_i)=^2-\x_i^TUU^T\x_i=-\Tilde{\x}_i^T\hat{U}\hat{U}^T\Tilde{\x}_i=-\left\Vert\Tilde{\x}_{i}\right\Vert^{2}\left\Vert \hat{U}_{i:}\right\Vert^2\Rightarrow - \frac{g(U)}{\left\Vert\x_{i}\right\Vert^{2}} = \left\Vert \hat{U}_{i:}\right\Vert^2
$$
(1- by \Lemmaref{local minimum equals}, 2- by \Lemmaref{local minimum orthogonal}), so
$$
r=\left\Vert \hat{U}\right\Vert^2_F=\sum_{i=1}^n \left\Vert \hat{U}_{i:}\right\Vert^2=\sum_{i=1}^n - \frac{g(U)}{\left\Vert\x_{i}\right\Vert^{2}}\Rightarrow g(U)= \frac{-r}{\sum_{i=1}^n \frac{1}{\left\Vert\x_{i}\right\Vert^{2}}}
$$
Finally, since all local minima have the same value, they are all global minima.
\end{proof}

\begin{claim} \label{has minimum}
$g$ has global minimum.
\end{claim}

\begin{proof}
    The set $D=\{U:\Vert U\Vert_F\le r\}\subset\mathbb{R}^{n\times r}$ is compact set, and $g:D\rightarrow\mathbb{R}$ is continuous function thus it has minimum. 
\end{proof}

\begin{corollary} \label{tagrget of original problem}
The objective of the optimal solution of \probref{factorized_problem} is: $\frac{r}{\sum_{i=1}^n \frac{1}{\left\Vert\x_{i}\right\Vert^{2}}}$.
\end{corollary}

\begin{proof}
    By \claimref{has minimum} $g$ has minimum, by \claimref{local minimum is global} the target of this minimum is: $\frac{-r}{\sum_{i=1}^n \frac{1}{\left\Vert\x_{i}\right\Vert^{2}}}$. By \Lemmaref{local minimum orthogonal} the optimal $U$ is in $\mathcal{O}^{n\times r}$ so we can use the equivalence between \probref{factorized_problem} and \probref{factorized_problem_restore}.
    
\end{proof}

\subsection{KKT conditions}


\subsection{SDR is tight}

\begin{claim} \label{SDR is tight}
SDR is tight.
\end{claim}
\begin{proof}
    In the SDR problem we solve:
    
\begin{align*}
    \max_{\begin{array}{c}
    Tr\left(P\right)\le r\\
    0\preceq P\preceq I
    \end{array}}\min_{i\in[n]}\x_{i}^TP\x_{i}
    =&\max_{\begin{array}{c}
    Tr\left(P\right)\le r\\
    0\preceq P\preceq I
    \end{array}}\min_{i\in[n]}\left\Vert \x_{i}\right\Vert^{2}\e_{i}^TVPV^T\e_{i}\\
    =& \max_{\begin{array}{c}
    Tr\left(VPV^T\right)\le r\\
    0\preceq VPV^T\preceq I
    \end{array}}\min_{i\in[n]}\left\Vert \x_{i}\right\Vert^{2}(VPV^T)_{ii}\\
    =&\max_{\begin{array}{c}
    Tr\left(\hat{P}\right)\le r\\
    0\preceq \hat{P}\preceq I
    \end{array}}\min_{i}\left\Vert \x_{i}\right\Vert^{2}\hat{P}_{ii}
\end{align*}

Where $VPV^T=\hat{P}$. Observe that if $\hat{P}$ is optimal then:
\begin{itemize}
    \item $\forall i,j:\quad\left\Vert\x_{i}\right\Vert^{2}\hat{P}_{ii}=\left\Vert\x_{j}\right\Vert^{2}\hat{P}_{jj}$, so for all $i$: $\hat{P}_{ii}=\frac{C}{\left\Vert\x_{i}\right\Vert^{2}}$ for some constant $C$.
    \item $r=\sum_{i=1}^n \hat{P}_{ii}=\sum_{i=1}^n\frac{C}{\left\Vert\x_{i}\right\Vert^{2}}$ so $C=\frac{r}{\sum_{i=1}^n\frac{1}{\left\Vert\x_{i}\right\Vert^{2}}}$.
\end{itemize}
i.e. the optimal solution is $\frac{r}{\sum_{i=1}^n\frac{1}{\left\Vert\x_{i}\right\Vert^{2}}}$, the same as the original problem (by \cororef{tagrget of original problem}) 

\end{proof}
It is worth mention that although the SDR is tight it does not promote low ranknes(!).

\subsection{PCA is sub-optimal}

Observe that in the PCA problem we sum over $i$ so we get:
\begin{align*}
    \sum_{i=1}^n\left\Vert\x_i-UU^T\x_i\right\Vert^2 - \left\Vert\x_{i}\right\Vert^2 =\sum_{i=1}^n-\x_i^TUU^T\x_i
    =\sum_{i=1}^n-\left\Vert U^T\x_{i}\right\Vert^2
    =\sum_{i=1}^n-\left\Vert\x_{i}\right\Vert^2\left\Vert \hat{U}_{i:}\right\Vert^2
\end{align*}
So we should assign norm of $1$ to the rows of $\hat{U}$ with the $r$ largest samples, i.e. we project to the subspace of the $r$ largest samples, so the value of the target for this $U$ is 0 since for $i=r+1...n$ we get: $x_iUU^Tx_i=0$. On the other hand, if we use the optimal $U$ we get the harmonic mean of the squared norms factorized by $\frac{r}{n}$ which is strictly greater then 0.


\subsection{Tight frame}

\begin{definition}.
\begin{itemize}
    \item Let $\{\u_i\}_{i=1}^n\subset\mathbb{R}^r$. If $span(\{\u_i\}_{i=1}^n)=\mathbb{R}^r$ then $\{\u_i\}_{i=1}^n$ is frame for $\mathbb{R}^r$.
    \item A frame $\{\u_i\}_{i=1}^n$ is tight with frame bound A if and only if $\forall \v\in\mathbb{R}^n$:
    $$\v=\frac{1}{A}\sum_{i=1}^n\left<\v,\u_i\right>\u_i$$
    \item We call $\{\u_i\}_{i=1}^n$ 'Normalized Tight Frame' if $\{\u_i\}_{i=1}^n$ is tight frame and $\Vert \u_i\Vert=1$ for all $i$.
\end{itemize}
\end{definition}

\begin{lemma}\label{standard basis enough}
A frame $\{\u_i\}_{i=1}^n\subset\mathbb{R}^r$ is tight with frame bound A if and only if $\forall \e_i$ in the standard basis of $\R^r$:
    $$\e_i=\frac{1}{A}\sum_{i=1}^n\left<\e_i,\u_i\right>\u_i$$
\end{lemma}

\begin{proof}
Observe that if $\;\forall \e_{i}\in\left\{ \e_{i}\right\} _{i=1}^{r}$ we have: $\e_{i}=\frac{1}{A}\sum_{j=1}^{n}\left\langle \e_{i},\u_{j}\right\rangle \u_{j}$ than we get for all $\v\in\mathbb{R}^{r}$:
\begin{align*}
    \v=	\sum_{j=1}^{r}\v_{j}\e_{j}
=	\sum_{j=1}^{r}\v_{j}\frac{1}{A}\sum_{i=1}^{n}\left\langle \e_{j},\u_{i}\right\rangle \u_{i}
=	\frac{1}{A}\sum_{i=1}^{n}\left\langle \sum_{j=1}^{r}\v_{j}\e_{j},\u_{i}\right\rangle \u_{i}
=	\frac{1}{A}\sum_{i=1}^{n}\left\langle \v,\u_{i}\right\rangle \u_{i}
\end{align*}
\end{proof}

\begin{claim} \label{tight frame iff scaled orthonormal}
Let $U^T=(\u_1,...,\u_n)\in\mathbb{R}^{r\times n}$. $\{\u_i\}_{i=1}^n$ is a tight frame with frame bound $A$ iff $U$ has orthogonal columns with norm $\sqrt{A}$. 
\end{claim}

\begin{proof}


Consider equality 1 below: 
\begin{align*}
    U^{T}U=\left(\u_{1},...,\u_{n}\right)\left(\u_{1},...,\u_{n}\right)^{T}=\left(\begin{array}{c}
    \\
    \sum_{j=1}^{n}\u_{j}^{1}\u_{j},\\
    \\
    \end{array}...,\begin{array}{c}
    \\
    \sum_{j=1}^{n}\u_{j}^{r}\u_{j}\\
    \\
    \end{array}\right)=^1\left(\begin{array}{c}
    \\
    A\e_{1}\\
    \\
    \end{array}...\begin{array}{c}
    \\
    A\e_{r}\\
    \\
    \end{array}\right)=AI
\end{align*}

Observe that $UU^T=AI$ iff $U$ has orthogonal columns with norm $\sqrt A$. Equality 1 also holds iff $A\e_{i}=\sum_{j=1}^{n}\u_{j}^{i}\u_{j}=\sum_{j=1}^{n}\left\langle \e_{i},\u_{j}\right\rangle \u_{j}$ which holds iff $\{\u_i\}_{i=1}^n$ is a tight frame with frame bound $A$ (by \Lemmaref{standard basis enough}), so we conclude that the conditions are equivalent.
\end{proof}

\begin{lemma} \label{sum is r}
 If for all k: $\left\Vert \x_k^TU\right\Vert^2=r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}$, then: $\norm{U}^2=r$.
\end{lemma}

\begin{proof}
$$
\left\Vert U\right\Vert_F^2=\left\Vert \hat{U}\right\Vert_F^2=\sum_{i=1}^n\norm{\e_i^T\hat{U}}^2=\sum_{i=1}^n\frac{1}{\norm{\Tilde{\x}_i}^2}\norm{\Tilde{\x}_i^T\hat{U}}^2=\sum_{i=1}^n\frac{1}{\norm{{\x}_i}^2}\norm{{\x}_i^T{U}}^2=\sum_{i=1}^n\frac{1}{\norm{{\x}_i}^2}r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}=r
$$
\end{proof}

\begin{claim} \label{tight frame iff optimal solution}
Let $U\in\mathbb{R}^{d\times r}$, the following conditions are equal:
\begin{itemize}
    \item $U^T$ is an optimal solution for \probref{factorized_problem_restore}.
    \item $U^T$ is tight frame such that for all k: $\left\Vert \x_k^TU\right\Vert^2=r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}$
\end{itemize}

\end{claim}
\begin{proof}

Let $U\in\mathbb{R}^{d\times r}$ an optimal solution for \probref{factorized_problem_restore}, by \Lemmaref{local minimum orthogonal} the columns of $U$ are orthonormal, so by \claimref{tight frame iff scaled orthonormal} $U$ is tight frame by \Lemmaref{local minimum is global} we have for all $k$:
$$
\norm{\x_k^TU}^2=f(U,\x_k)=r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}
$$
On the other hand, let $U^T$ a tight frame as above, by \claimref{tight frame iff scaled orthonormal} the columns of $U$ are orthogonal and have the same norm. By \Lemmaref{sum is r} $\norm{U}_F^2=r$ so the columns has unit norms, i.e. the columns are orthonormal and $U$ is feasible solution. Finally, for all $i\in[n]$:
\begin{align*}
    f(U,\x_i)
    =-\x_{i}^TUU^T\x_{i}
    =-r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}
\end{align*}
i.e. $\min_i\left\Vert \x_i-UU^T\x_i\right\Vert^2-\left\Vert \x_{i}\right\Vert^{2}=-r\left({\sum_{i=1}^n \frac{1}{\left\Vert \x_{i}\right\Vert^{2}}}\right)^{-1}$ which is the optimal target.
\end{proof}

\begin{corollary}
If $\forall i:\; \x_i=\e_i$, then \probref{factorized_problem} is reduced to the problem of finding 'normalized tight frame'.
\end{corollary}


\begin{claim} \label{optimization tight frame}
Denote $H=n\left(\sum_{i=1}^{n}\frac{1}{\left\Vert \x_{i}\right\Vert ^{2}}\right)^{-1}$ (the harmonic mean of the samples), then:

\begin{align} \label{multicast formulation}
    \arg\min_{\begin{array}{c}
    \forall k:\;\left\Vert \x_{k}U\right\Vert ^{2}=\frac{r}{n}H
    \end{array}}\norm{U^TU}^2_F
\end{align}
is an optimal solution for \probref{factorized_problem}.
\end{claim}

\begin{proof}
By \Lemmaref{sum is r} we know that any feasible solution has: $\left\Vert U\right\Vert _{F}^{2}=r$ so the problem is equivalent to: 
\begin{align*}
    \min_{\begin{array}{c}
    \forall k:\;\left\Vert \x_{k}U\right\Vert ^{2}=\frac{r}{n}H\\
    \norm{U}^2_F=r
    \end{array}}\norm{U^TU}^2_F
\end{align*}
Consider the following relaxations of the problem:
\begin{align*}
    \min_{\begin{array}{c}
    \norm{U}^2_F=r
    \end{array}}\norm{U^TU}^2_F
    =
    \min_{\begin{array}{c}
    \sum_{i=1}^r\lambda_i=r\\
    \lambda_i \;is\; eigenvalue\; of\; UU^T
    \end{array}}\sum_{i=1}^r\lambda_i^2
    \ge
    \min_{\begin{array}{c}
    \sum_{i=1}^r\lambda_i=r
    \end{array}}\sum_{i=1}^r\lambda_i^2
\end{align*}
A necessary condition for optimality of the right relaxation is:
$$
\nabla_\lambda\mathcal{L}=\nabla_\lambda \sum_{i=1}^r\lambda_i^2+\mu(\sum_{i=1}^r\lambda_i-r)=0\Leftrightarrow\forall i:\;2\lambda_i=\mu\Leftrightarrow\forall ij:\;\lambda_i=\lambda_j\Leftrightarrow\forall i:\;\lambda_i=1
$$


So a necessary condition for the optimality of the left relaxation is that $\lambda_i=1$ for all $i\in[r]$, i.e. $U$ has orthonormal columns. Finally, the orthonormality constraint and the norm constraint is satisfied iff $U$ is an optimal solution for \probref{factorized_problem} so the problems are equivalent.
\end{proof}

\begin{corollary}
If $r=1$ then \probref{multicast formulation} is reduced to:
\begin{align*} 
    \arg\min_{\begin{array}{c}
    \forall k:\;\left\Vert \x_{k}\u\right\Vert ^{2}\ge\frac{1}{n}H\\
    \end{array}}\u^T\u
\end{align*}
which we already know as multicast beamforming.
\end{corollary}

\subsection{Appendix} \label{technical proof}
\begin{theorem}
Let $U\in\mathbb{R}^{n\times r}$, $R=G(\theta,i,j)$ (a Given rotation over the axes i,j), $U^{'}=RU$.
\begin{align*}
    \left\Vert \e_{i}-U^{'}U^{'T}\e_{i}\right\Vert ^{2}-\left\Vert \e_{i}-UU^{T}\e_{i}\right\Vert ^{2}= \sin\left(2\theta\right)\e_{i}^{T}UU^{T}\e_{j}^{T}+\sin\left(\theta\right)^{2}\left(\left\Vert \e_{j}-UU^{T}\e_{j}\right\Vert ^{2}-\left\Vert \e_{i}-UU^{T}\e_{i}\right\Vert ^{2}\right)
\end{align*}
\end{theorem}
\begin{proof}
Let $ODO^{T}$ an EVD decomposition of $UU^{T}$, and denote $\hat{D}:=D^{2}-2D$ then:

\begin{align*}
    \left\Vert \x_{i}-UU^{T}\x_{i}\right\Vert ^{2}=&	\left\Vert \x_{i}\right\Vert ^{2}-2\x_{i}^{T}UU^{T}\x_{i}+\x_{i}^{T}UU^{T}UU^{T}\x_{i}\\
=&	\left\Vert \x_{i}\right\Vert ^{2}-2\x_{i}^{T}ODO^{T}\x_{i}+\x_{i}^{T}OD^{2}O^{T}\x_{i}\\
=&	\left\Vert \x_{i}\right\Vert ^{2}+\x_{i}^{T}O\left(D^{2}-2D\right)O^{T}\x_{i}\\
=&	\left\Vert \x_{i}\right\Vert ^{2}+\sum_{l=1}^{r}\hat{D}_{ll}\left[O^{T}\x_{i}\right]_{l}^{2}\\
=&  \left\Vert \x_{i}\right\Vert ^{2}\left(1+\sum_{l=1}^{r}\hat{D}_{ll}O_{li}^{2}\right)
\end{align*}
and since $RODO^{T}R^T$ is the EVD decomposition of $U^{'}U^{'T}$, we also have: 
\begin{align*}
    \left\Vert \x_{i}-U^{'}U^{'T}\x_{i}\right\Vert^{2}
    =& \left\Vert \x_{i}\right\Vert^{2}+\sum_{l=1}^{r}\hat{D}_{ll}\left[O^{T}R^T\x_{i}\right]_{l}^{2}\\
    =& \left\Vert \x_{i}\right\Vert^{2}\left(1+\sum_{l=1}^{r}\hat{D}_{ll}\left[O^{T}R^T\e_{i}\right]_{l}^{2}\right)\\
    =& \left\Vert \x_{i}\right\Vert^{2}\left(1+\sum_{l=1}^{r}\hat{D}_{ll}\left[O^{T}\left(\cos\left(\theta\right)\e_{i}+\sin\left(\theta\right)\e_{j}\right)\right]_{l}^{2}\right)\\
    =& \left\Vert \x_{i}\right\Vert^{2}\left(1+\sum_{l=1}^{r}\hat{D}_{ll}\left(\cos\left(\theta\right)^2O_{li}^{2}+2\cos\left(\theta\right)O_{li}\sin\left(\theta\right)O_{lj}+\sin\left(\theta\right)^{2}O_{lj}^{2}\right)\right)
\end{align*}
So:
\begin{align*}
    \left\Vert \e_{i}-U^{'}U^{'T}\e_{i}\right\Vert ^{2}-\left\Vert \e_{i}-UU^{T}\e_{i}\right\Vert^{2}=&\sum_{l=1}^{r}\hat{D}_{ll}\left(2\cos\left(\theta\right)O_{li}\sin\left(\theta\right)O_{lj}+\sin^{2}\left(\theta\right)O_{lj}^{2}-\left(1-\cos^{2}\left(\theta\right)\right)O_{li}^{2}\right)\\
    =& \sum_{l=1}^{r}\hat{D}_{ll}\left(2\cos\left(\theta\right)O_{li}\sin\left(\theta\right)O_{lj}+\sin^{2}\left(\theta\right)O_{lj}^{2}-\sin^{2}\left(\theta\right)O_{li}^{2}\right)\\
    =& \sum_{l=1}^{r}\hat{D}_{ll}\left(2\cos\left(\theta\right)O_{li}\sin\left(\theta\right)O_{lj}+\sin^{2}\left(\theta\right)\left(O_{lj}^{2}-O_{li}^{2}\right)\right)\\
    =& 2\cos\left(\theta\right)\sin\left(\theta\right)\sum_{l=1}^{r}\hat{D}_{ll}O_{li}O_{lj}+\sin^{2}\left(\theta\right)\sum_{l=1}^{r}\hat{D}_{ll}\left(O_{lj}^{2}-O_{li}^{2}\right)\\
    =& \sin\left(2\theta\right)\e_{i}^{T}UU^{T}\e_{j}^{T}+\sin^{2}\left(\theta\right)\left(\left\Vert \e_{j}-UU^{T}\e_{j}\right\Vert^{2}-\left\Vert \e_{i}-UU^{T}\e_{i}\right\Vert ^{2}\right)
\end{align*}
\end{proof}


\end{document}
